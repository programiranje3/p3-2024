{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory data analysis\n",
    "Introduction to exploratory data analysis (EDA)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "EDA is an approach to analyzing datasets to summarize their main characteristics, often with visual methods. EDA is used for seeing what the data can tell us before the modeling task [(source 1)](https://chartio.com/learn/data-analytics/what-is-exploratory-data-analysis/). It is used to explore the data, find different patterns, relations, and anomalies in the data using some statistical graphs and other visualization techniques, and possibly formulate hypotheses that could lead to new data collection and experiments [(source 2)](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/). More specifically, EDA enables analysts to:\n",
    "1. get maximum insights from a data set\n",
    "2. uncover underlying structure\n",
    "3. extract important variables from the dataset\n",
    "4. detect outliers and anomalies (if any)\n",
    "5. test underlying assumptions\n",
    "6. determine the optimal factor settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA steps and tools\n",
    "Practical steps in conducting EDA and frequently used EDA tools.\n",
    "Based on *pandas2020-main.Sales_Analysis_Pandas_P3_tutorial.ipynb* and *pandas2020-main.TED_Talks_Pandas_P3_tutorial.ipynb*.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on [this](https://stackoverflow.com/a/22149930/1899061), in all computations, `axis=...` refers to the axis **along which** the computation is done. By default, `axis=0`. This is consistent with the `numpy.mean` usage when axis is specified explicitly (in `numpy.mean`, `axis==None` by default, which computes the mean value over the flattened array), in which `axis=0` along the rows (namely, index in pandas), and `axis=1` along the columns.\n",
    "Note also that that `axis=0` indicates aggregating along rows and `axis=1` indicates aggregating along columns. This is consistent with how we index into a dataframe. In `df.iloc[<row>, <column>]`, `<row>` is in index position 0 and `<column>` is in index position 1. For added clarity, one may choose to specify `axis='index'` (instead of `axis=0`) or `axis='columns'` (instead of `axis=1`).\n",
    "**But**, `axis=0` means each row as a bulk - we manipulate a `pd.DataFrame` inter-row, instead of within-row. Likewise, 1 means each column as a bulk, i.e. we manipulate a `pd.DataFrame` inter-column instead of within-column. For example, `<pd.df>.drop(\"A\", axis=1)` will drop a whole column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the dataset\n",
    "- `pd.read_csv()`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial examination and adaptations\n",
    "- `<pd.df>.shape`, `<pd.df>.head()`, `<pd.df>.tail()`, `<pd.df>.sample()`, `<pd.df>.dtypes`, `<pd.df>.info()`, `<pd.df>.describe()`\n",
    "- `<pd.df>.loc[...]`, `<pd.df>.iloc[...]` - examine individual cells, columns, rows\n",
    "    - `loc` works with conditions and column names, `iloc` with numerical indices\n",
    "    - in both `loc` and `iloc`, multiple columns can be specified as a list of column names, and `:` in each index position means 'all'\n",
    "    - in `iloc`, both index positions can be specified as lists of numeric values\n",
    "- `<pd.df>.columns`, `<pd.df>.columns.values`, `<pd.df>.columns.values.tolist()` (or `<pd.df>.columns.values.to_list()`), `<pd.df>.values`\n",
    "- `<pd.df>.rename({'<column_1 old name>':'<column_1 new name>', '<column_2 old name>':'<column_2 new name>', ...}, axis='columns')`, `<pd.df>.columns = ['<column_1 name>', '<column_2 name>', ...]` (change the names of all columns in <pd.df>)\n",
    "- `ast.literal_eval()` (using Python's *ast* module to transform a string into a literal value, a list, a tuple or any other container object)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing values and value counts\n",
    "- `sb.heatmap()`, e.g. `sb.heatmap(<pd.df>.isna(),cbar=False,cmap='viridis')` ([example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/))\n",
    "- `<pd.df>.isna()` (`<pd.df>.isnull()`), `<pd.df>.isna().sum()` (`<pd.df>.isnull().sum`) ([example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/))\n",
    "- `<pd.df>['<column>'].value_counts()` (shows only the rows without NAs (default: dropna=True), check shape)\n",
    "- `<pd.df>['<column>'].value_counts(normalize=True)` (show proportions, rather than frequencies)\n",
    "- `<pd.Series>.dropna(how='all'/'any', inplace=True)` (return a new `<pd.Series>` object with missing values removed)\n",
    "\n",
    "The `cmap` parameter of `sb.heatmap()` denotes a [Matplotlib colormap](https://matplotlib.org/stable/tutorials/colors/colormaps.html#classes-of-colormaps) (`viridis`, `cividis`, `tab20`, `winter`, `BuPu_r`, `ocean`,...)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examining individual data items, rows and columns\n",
    "- `<pd.df>.sample()`\n",
    "- Simple indexing and fancy indexing: `<pd.df>.iloc[]`, `<pd.df>.loc[]`\n",
    "- `<pd.df>.index`, `<pd.df>.index[<from>:<to>]`, `<pd.df>.reset_index(drop=True, inplace=True)`\n",
    "- Indexing using list of values: `<pd.df>.loc[<pd.df>.<column>.isin(<list of values>)]` (select those observations where the value of `<column>` is in the `<list of values>`)\n",
    "- Indexing in data stats: `<pd.df>.describe().loc['50%', '<column_name>']` (select the median of `<column_name>` from the `<pd.df>` stats computed by `describe()`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping and sorting data\n",
    "- `<pd.df>['<column>'].unique()`, `<pd.df>['<column>'].nunique()`\n",
    "- `<pd.df>['<column>'].groupby()`, `<pd.df>['<column>'].groupby().get_group()`\n",
    "- `<pd.df>['<column>'].value_counts()`, `<pd.df>['<column>'].value_counts().sort_index()`, `<pd.df>['<column>'].value_counts().sort_index(inplace=True)`\n",
    "- `<pd.df>.sort_values(by='<column name>', ascending=False/True)`\n",
    "- `<pd.df>.groupby('<column>').<another column>.<f()>.sort_values(ascending=False)` (aggregate using function `f()`, e.g. `mean()`)\n",
    "- `<pd.df>.groupby('<column>').<another column>.agg(['<f1 name>', '<f2 name>', ...])` (aggregate using multiple functions, e.g. `mean()`, `count()`,...)\n",
    "\n",
    "If `sort_values()` is used after `agg(['f1 name>', '<f2 name>', ...])` (`agg(['<f1 name>', '<f2 name>', ...]).sort_values(by='<f name>', ascending=False)`), it must be passed one positional argument (`by='<f name>'`) before the optional `ascending=False`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data transformations\n",
    "- `<pd.df>.describe()`\n",
    "- `pd.to_numeric(<pd.DataFrame object>['<column name>'], errors='coerce')`, `pd.DataFrame.to_numpy()`, `pd.Series.to_numpy()`, `pd.to_datetime()`, ...\n",
    "- `<pd.df>.<column>.apply(<f_name>)` (apply the <f_name> function to all elements of each element of the `<column>`; for example, each element of the `<column>` can be a list of other elements)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring correlations\n",
    "Explore correlations between the (numerical) columns. \n",
    "- `sb.heatmap()`\n",
    "- [Example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data visualization\n",
    "Plot some bargraphs, scatterplots, boxplots,...\n",
    "- [Example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other\n",
    "[Other interesting ideas and different ways of using the things from above](https://realpython.com/pandas-python-explore-dataset/#exploring-your-dataset) (see the rest from [that article](https://realpython.com/pandas-python-explore-dataset/) as well)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import and configure packages\n",
    "The `%run` magic might not work well in DataSpell, thus the following `import` statements are copied here from *import_packages.ipynb*:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "%run \"../notebooks/import_packages.ipynb\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # %load_ext autoreload\n",
    "# # %autoreload 2\n",
    "# \n",
    "# %matplotlib inline\n",
    "# \n",
    "# # %config IPCompleter.greedy=True\n",
    "# \n",
    "# import numpy as np\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('classic')\n",
    "# import pandas as pd\n",
    "# import seaborn as sb\n",
    "# \n",
    "# from plotnine import ggplot, aes, labs, geom_point, geom_line, geom_histogram, theme_xkcd, coord_cartesian, xlim, ylim, xlab, ylab, ggtitle, theme"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Introducing grunge datasets",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Available datasets\n",
    "Relevant datasets, located in the *data* folder:\n",
    "* *grunge.csv* - complete raw dataset, located in the *data* folder; copied directly from [this Kaggle dataset](https://www.kaggle.com/datasets/anavui/grunge-bands-dataset), developed by [Ana Vucic](https://www.kaggle.com/anavui)\n",
    "* *pearl_jam_v1.csv*, *pearl_jam_v2.csv*, *pearl_jam_v3.csv* - incomplete raw Pearl Jam datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read the *csv* file containing one of the available datasets describing grunge songs\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the songs as a pd.DataFrame object from 'data/grunge_v1.csv', \n",
    "# or from '../data/grunge_v1.csv', \n",
    "# or '../../data/grunge_v1.csv', \n",
    "# or ..., \n",
    "# depending on where the csv file is located\n",
    "\n",
    "# If an int column contains NaN values, read_csv() sets all values to float values, because NaN are internally\n",
    "# represented as float values. To read the int columns as int values and still preserve NaN values where they \n",
    "# exist, see this: https://stackoverflow.com/a/72323514. \n",
    "# The trick is: df = pd.read_csv('file.csv', dtype={'a': 'Int32', 'b': 'Int32'}), assuming that 'a' and 'b' \n",
    "# columns contain int and NaN values.\n",
    "\n",
    "songs = pd.read_csv('../data/grunge_v1.csv')\n",
    "songs"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explore the dataset (first steps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initial examination and adaptations\n",
    "- `<pd.df>.shape`, `<pd.df>.head()`, `<pd.df>.tail()`, `<pd.df>.sample()`, `<pd.df>.dtypes`, `<pd.df>.info()`, `<pd.df>.describe()`\n",
    "- `<pd.df>.loc[...]`, `<pd.df>.iloc[...]` - examine individual cells, columns, rows\n",
    "    - `loc` works with conditions and column names, `iloc` with numerical indices\n",
    "    - in both `loc` and `iloc`, multiple columns can be specified as a list of column names, and `:` in each index position means 'all'\n",
    "    - in `iloc`, both index positions can be specified as lists of numeric values\n",
    "- `<pd.df>.columns`, `<pd.df>.columns.values`, `<pd.df>.columns.values.tolist()` (or `<pd.df>.columns.values.to_list()`), `<pd.df>.values`\n",
    "- `<pd.df>.rename({'<column_1 old name>':'<column_1 new name>', '<column_2 old name>':'<column_2 new name>', ...}, axis='columns')`, `<pd.df>.columns = ['<column_1 name>', '<column_2 name>', ...]` (change the names of all columns in <pd.df>)\n",
    "- `ast.literal_eval()` (using Python's *ast* module to transform a string into a literal value, a list, a tuple or any other container object)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### A sneak peek into the dataset\n",
    "- `<pd.df>.shape`, `<pd.df>.head()`, `<pd.df>.tail()`, `<pd.df>.sample()`, `<pd.df>.dtypes`, **<u>`<pd.df>.info()`**</u>, `<pd.df>.describe()` (shows descriptive statistics for numerical columns only).\n",
    "\n",
    "When calling `display()` on a method like `<pd.df>.head()`, `<pd.df>.tail()` and `<pd.df>.sample()`, only a certain default number of columns is displayed. To display *all* columns, use `pd.set_option('display.max_columns', None)` first. To display `<n>` columns, use `pd.set_option('display.max_columns', <n>)` first. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "songs\n",
    "songs.tail()\n",
    "songs.head()\n",
    "songs.sample(10)\n",
    "songs.info()\n",
    "songs.describe()\n",
    "songs.dtypes\n",
    "songs.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Columns\n",
    "- `<pd.df>.columns`, `<pd.df>.columns.values`, `<pd.df>.columns.values.tolist()` (or `<pd.df>.columns.values.to_list()`), `<pd.df>.values`\n",
    "\n",
    "Show the columns of the `songs` object (which is a `pd.DataFrame` object)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the columns as a pd.Index object, using <pd.df>.columns\n",
    "songs.columns\n",
    "# Get the columns as a list, using list(<pd.df>.columns)\n",
    "list(songs.columns)\n",
    "# Get the columns as a list, using <pd.df>.columns.tolist() or <pd.df>.columns.to_list()\n",
    "songs.columns.tolist()\n",
    "# Get the columns as a numpy.ndarray object, using <pd.df>.columns.values or np.array(<pd.df>.columns)\n",
    "songs.columns.values\n",
    "# Get the values of all items in the dataset as a numpy.ndarray of sequences of the values in each item, \n",
    "# using <pd.df>.values (the type of both the encompassing and the encompassed sequences is numpy.ndarray)\n",
    "songs.values"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Renaming columns\n",
    "- `<pd.df>.rename(columns={'<column_1 old name>':'<column_1 new name>', '<column_2 old name>':'<column_2 new name>', ...}, inplace=True)`, or\n",
    "- `<pd.df>.rename({'<column_1 old name>':'<column_1 new name>', '<column_2 old name>':'<column_2 new name>', ...}, axis='columns', inplace=True)`;\n",
    "- `<pd.df>.columns = ['<column_1 name>', '<column_2 name>', ...]` (change the names of all columns in `<pd.df>`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Rename the names of some columns\n",
    "songs.rename({'album_name': 'album', 'track_name': 'track', 'producers': 'produced_by'}, axis='columns', inplace=True)\n",
    "songs.columns.tolist()\n",
    "# Rename these columns back to their original names\n",
    "songs.rename({'produced_by': 'producers'}, axis='columns', inplace=True)\n",
    "songs.columns.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Rearranging columns\n",
    "- `<pd.df> = <pd.df>[['<col to be the first>', 'col to be the second'...etc.]]`\n",
    "- `column_names = ['<col to be the first>', 'col to be the second'...etc.]`, `<pd.df> = <pd.df>.reindex(columns=column_names)`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['track', 'artists', 'album', 'album_type', 'duration', 'release_year',]\n",
    "columns.extend(songs.columns[6:])\n",
    "songs = songs.reindex(columns=columns)\n",
    "songs.columns\n",
    "songs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the modified dataset\n",
    "songs.to_csv('../data/grunge_v2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Missing values and value counts\n",
    "- `sb.heatmap()`, e.g. `sb.heatmap(<pd.df>.isna(),cbar=False,cmap='viridis')` ([example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/))\n",
    "- `<pd.df>.isna()` (`<pd.df>.isnull()`), `<pd.df>.isna().sum()` (`<pd.df>.isnull().sum()`) ([example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/))\n",
    "- `<pd.df>['<column>'].value_counts()` (shows only the rows without NAs (default: dropna=True), check shape)\n",
    "- `<pd.df>['<column>'].value_counts(normalize=True)` (show proportions, rather than frequencies)\n",
    "- `<pd.df>.dropna(how='all'/'any', inplace=True)`, `<pd.Series>.dropna(how='all'/'any', inplace=True)` (return a new `<pd.Series>`/`<pd.Series>` object with missing values removed)\n",
    "\n",
    "The `cmap` parameter of `sb.heatmap()` denotes a [Matplotlib colormap](https://matplotlib.org/stable/tutorials/colors/colormaps.html#classes-of-colormaps) (`viridis`, `cividis`, `tab20`, `winter`, `BuPu_r`, `ocean`,...)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the dataset\n",
    "songs = pd.read_csv('../data/grunge_v2.csv')\n",
    "songs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the heatmap (missing values) of the songs dataset \n",
    "# (demonstrate using sb.heatmap(songs.isna(), cbar=False, cmap='cividis') vs. \n",
    "# sb.heatmap(songs.isna(), cbar=False, cmap='cividis');)\n",
    "sb.heatmap(songs.isna(), cbar=False, cmap='cividis');"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "How many missing values are there? (`<pd.df>.isna().sum()` for all columns, `<pd.df>.['<column>'].isna().sum()` for a specific column, `<pd.df>.isna()[['<column1>', 'column2', ...]].sum()` for selected multiple columns; `isnull()` is the same as `isna()`, and `isna()` is used more often).\n",
    "\n",
    "Try also `<pd.df>.isna()`, `<pd.df>.isna()[['<column1>', 'column2', ...]]`, `type(<pd.df>.isna())`, `type(<pd.df>.isna().sum())`, `type(<pd.df>.isna()[['<column1>', 'column2', ...]].sum())`, `<pd.df>.isna().sum().value_counts()`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "songs.isna().sum()\n",
    "songs.album_moods.isna().sum()\n",
    "songs.isna().sum()[['acousticness', 'album_moods']]\n",
    "type(songs.isna().sum())"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "How many missing values are there in the columns where there *are* missing values? `<i> = <pd.df>.isna().sum() > 0`, `<pd.df>.isna().sum()[<i>]`. \n",
    "Try also `<i>`, `type(<i>)`, `<i>[<i>]`, `<pd.df>.loc[:, <i>]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "i = songs.isna().sum() > 0\n",
    "i\n",
    "i[i]\n",
    "songs.isna().sum()\n",
    "songs.isna().sum()[i]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Leave out rows with `np.NaN` values: `<pd.df>.dropna()`, `<pd.df>.<column>.dropna()`, `<pd.df>['<column>'].dropna()`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "songs.dropna()\n",
    "songs.album_moods.dropna()\n",
    "songs[['album_moods', 'album_genres']].dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Leave out columns with `np.NaN` values: `<pd.df>.dropna(axis=1)` (or `<pd.df>.dropna(axis=columns)`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "songs.dropna(axis=1, inplace=True)\n",
    "songs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the modified dataset\n",
    "songs.to_csv('../data/grunge_v3.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Value counts\n",
    "Show value counts for a dataframe: `<pd.df>.value_counts()`, `<pd.df>.value_counts(normalize=True)`.\n",
    "- `<pd.df>['<column>'].value_counts()` (shows only the rows without NAs (default: dropna=True), check shape)\n",
    "- `<pd.df>['<column>'].value_counts(normalize=True)` (show proportions, rather than frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "songs.value_counts()\n",
    "songs.track.value_counts()\n",
    "# songs.track.value_counts(normalize=True)\n",
    "songs.track.value_counts().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Duplicates\n",
    "The `<pd.df>.duplicated()` method returns a `pd.Series` object with boolean values denoting duplicated rows. The removal of duplicated rows is done by `<pd.df>.drop_duplicates()` (see below).\n",
    "\n",
    "Find/Show *complete* duplicates (if any): `<pd.df>.duplicated()` (keeps the first occurrence by default, i.e. marks duplicates as `True` except for the first occurrence), `<pd.df>.duplicated(keep='last)` (keeps the last occurrence by default, i.e. marks duplicates as `True` except for the last occurrence), `<pd.df>.duplicated(keep=False)` (marks all occurrences as `True`).\n",
    "\n",
    "Find duplicates based on a specific column): `<pd.df>.<column>.duplicated()`, `<pd.df>.duplicated(subset=['<column>'])`.\n",
    "\n",
    "Find duplicates based on multiple specific columns: `<pd.df>.duplicated(subset=['<column 1>', '<column 2>',...])`. \n",
    "\n",
    "Drop *complete* duplicates (if any): `<pd.df>.drop_duplicates(inplace=True)`.\n",
    "\n",
    "Remove rows with duplicates in a certain column (if any), keeping the first row with that value in that column (default): `<pd.df>.drop_duplicates('<column>', inplace=True)`, or keeping the last row with that value in that column: `<pd.df>.drop_duplicates('<column>', keep='last', inplace=True)`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show duplicated songs (complete duplicates)\n",
    "songs.duplicated()\n",
    "songs.track.duplicated()\n",
    "songs.track.duplicated().value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show duplicates based on the song title (the 'track' column) only\n",
    "songs.track.duplicated()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the effect of removing duplicates based on the song title (the 'track' column), i.e. without inplace=True\n",
    "songs.track.drop_duplicates()\n",
    "# songs.track.drop_duplicates(keep='last')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Removing the rows with a specific value (specific values) in a certain column\n",
    "Use: `<pd.df> = <pd.df>[<pd.df>.<column> == <value>, inplace=True)`, `<pd.df> = <pd.df>[<pd.df>['<column>'] == <value>, inplace=True)`, `<pd.df> = <pd.df>[<pd.df>.<column> != <value>, inplace=True)`, `<pd.df> = <pd.df>[<pd.df>['<column>'] != <value>, inplace=True)`, `<pd.df>[<pd.df>.<column>.isin([<value 1>, <value 2>, ...])]`, and the like."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove the songs appearing on live, compilation, and box set albums, using\n",
    "# <pd.df>[<pd.df>.<column>.isin([<value 1>, <value 2>, ...])]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Check for duplicates now; if there ARE duplicates left, examine them and their features manually\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Reset index to 0:n: <pd.df>.reset_index(drop=True, inplace=True); drop=True: do not insert index into columns\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove the remaining duplicate: <pd.df>.drop_duplicates('track', keep='first'|'last', inplace=True);\n",
    "# alternatively, <pd.df>.drop(index=<i>, inplace=True), <pd.df>.drop(index=[<i1>, <i2>, ...], inplace=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Optionally, reset index again\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Save the modified dataset\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examining individual data items, rows and columns\n",
    "- `<pd.df>.sample()`\n",
    "- Simple indexing and fancy indexing: `<pd.df>.iloc[]`, `<pd.df>.loc[]`\n",
    "- `<pd.df>.index`, `<pd.df>.index[<from>:<to>]`, `<pd.df>.reset_index(drop=True, inplace=True)`\n",
    "- Indexing using list of values: `<pd.df>.loc[<pd.df>.<column>.isin(<list of values>)]` (select those observations where the value of <column> is in the `<list of values>`)\n",
    "- Indexing in data stats: `<pd.df>.describe().loc['50%', '<column_name>']` (select the median of `<column_name>` from the `<pd.df>` stats computed by `describe()`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Read a dataset with missing values."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a sample of the dataset to get a feeling of what's in there."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "What are the songs that have *some* missing values? \n",
    "Use masking to create the index of such elements; e.g. `<i>`, e.g., `<i> = songs.isna().sum() > 0` and show the type of the result (it's a `pd.Series` object).\n",
    "Display `<i>.index` and `<i>.values`. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From the `pd.Series` object `<i>` created in the previous step, select the elements that have the values > 0 (i.e., the names of the columns that have some `NaN` values) - `<i>[<i>]`, `<i>[<i>.values > 0]` (works because `<i>.values` are `True` and `False`, and `True.__int__() = 1`).\n",
    "\n",
    "Also, from the `<pd.df>` select a subset with only those columns that have *some* `NaN` values - `<pd.df>.loc[:, <i>]`."
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "How many `NaN`s are there in each column that has `NaN`s? `<pd.df>.isna().sum()[i]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "From a `<pd.df>` select all rows that have *some* missing values: `<pd.df>[<pd.df>.isna().any(axis=1)]`, `<pd.df>.loc[<pd.df>.isna().any(axis=1)]`, `<pd.df>.loc[<pd.df>.isna().any(axis=1), :]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Select rows based on column conditions: `<pd.df>.loc[<pd.df>.<column 1> == <...>]`, `<pd.df>.loc[(<pd.df>.<column 1> == <...>) & (<pd.df>.<column 2> == <...>)]`, etc. Notice the use of `&`, not `and`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "What are the rows that have missing values in a specific column of a `<pd.df>`? For example, what are the songs with missing `track_composers` values?",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using `isna()`, `loc[]`, `iloc[]`, `len()` and `index`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calling `loc[]` effectively means *creating a subset* (typically based on a relational or logical expression over one or more columns of the dataset). In other words, `loc[]` creates a *slice* of the dataframe, so the type of the result is `<pd.df>`.\n",
    "\n",
    "Note that `loc[]` works as `loc[<selected rows>, <selected columns>]`. The indices `<selected rows>` and `<selected columns>` can be created either directly in `loc[]` or beforehand.\n",
    "\n",
    "If defining the <selected rows> index to be used with `loc[]` subsequently, it is a good practice to define it as a boolean *mask* over a single column, like `<pd.df>['<column>'].isna()`, or as a logical expression in which each chunk is a relational expression over a single column, e.g. `<pd.df>['<column1>'].isna() & <pd.df>['<column2>'] < 23`. The result will be a subset of the original dataframe (i.e., another `<pd.df>`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining the relevant index with a statement like `<pd.df>.loc[<pd.df>['<column>'].isna()].index` is a good starting point when using `iloc[]` subsequently.\n",
    "\n",
    "If using `iloc[]`, don't forget the `.index` chunk in the statement used to create the index (such as `<pd.df>.loc[<pd.df>['<column>'].isna()].index`). Without it, the result is another `<pd.df>`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define i_iloc, the index to be used with iloc[], starting from <i> = <pd.df>['<column>'].isna();\n",
    "# iloc[] can be used conveniently here if the relevant index is already defined with <pd.df>.loc[<i>].index, i.e. <pd.df>.loc[<pd.df>['<column>'].isna()].index;\n",
    "# remember that the second index in iloc[] must be a number too (the relevant column index)\n",
    "\n",
    "# Define i_loc, the index (boolean mask) to be used with loc[], e.g. i_loc = <pd.df>['<column>'].isna()\n",
    "\n",
    "# display(songs.loc[i_loc.index, ['track', 'track_composers']])\n",
    "# display(songs.iloc[i_iloc, [0, 20]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Replace `NaN` values in `track_composers` with `'unknown'` (`<pd.df>.loc[<i_loc>, '<column>'] = <new value>`, `<pd.df>.iloc[<i_iloc>, <column index>] = <new value>`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Make the replacement and display it\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Double-check the missing values now:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use <pd.df>.<column>.isna().sum(), or <pd.df>.isna().sum()['<column>'], or \n",
    "# sb.heatmap(<pd.df>.isna(), cmap='...')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "How many songs from the beginning of the grunge era are there?",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the beginning of the grunge era as a list comprehension\n",
    "\n",
    "# Display the songs from the early years using a combination of <pd.df>.loc[] and isin()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping and sorting data\n",
    "- `<pd.df>['<column>'].unique()`, `<pd.df>['<column>'].nunique()`\n",
    "- `<pd.df>.<column>.groupby()`, `<pd.df>.groupby('<column>')`, `<pd.df>.groupby('<column>').get_group(<value>)`\n",
    "- `<pd.df>['<column>'].value_counts()`, `<pd.df>['<column>'].value_counts().sort_index()`, `<pd.df>['<column>'].value_counts().sort_index(inplace=True)`\n",
    "- `<pd.df>.sort_values(by='<column name>', ascending=False/True)`\n",
    "- `<pd.df>.groupby('<column>').<another column>.<f()>.sort_values(ascending=False)` (aggregate using function `f()`, e.g. `mean()`)\n",
    "- `<pd.df>.groupby('<column>').<another column>.agg(['<f1 name>', '<f2 name>', ...])` (aggregate using multiple functions, e.g. `mean()`, `count()`,...)\n",
    "\n",
    "If `sort_values()` is used after `agg([<'f1 name>', '<f2 name>', ...])` (`agg(['<f1 name>', '<f2 name>', ...]).sort_values(by='<f name>', ascending=False)`), it must be passed one positional argument (`by='<f name>'`) before the optional `ascending=False`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Read a dataset without missing values."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How many unique values for `release_year` are there in the dataset (`<pd.df>['<column>'].unique()`, `<pd.df>.<column>.unique()`; `<pd.df>['<column>'].nunique()`, `<pd.df>.<column>.nunique()`)?"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group the songs in the dataset by the year of release (`<pd.df>.groupby('<column>')`). The result can be `songs_by_year`. Display it, show its type, and explore its individual groups and their types (`<pd.df>.groupby('<column>').get_group(<value>)`). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How many songs are there in the dataset for each `release_year` (`<pd.df>['<column>'].value_counts()`, `<pd.df>['<column>'].value_counts()[<year>]`, `<pd.df>['<column>'].value_counts().sort_index()`)?\n",
    "\n",
    "Note that `value_counts()` returns a `pd.Series` object, with the index equal to `<pd.df>['<column>'].unique()` values."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sort the songs from the dataset by the year of release (`<pd.df>.sort_values(by='<column name>', ascending=False/True)`).\n",
    "(It is also possible to use `inplace=True` in `sort_values()`, but it will change the order of songs in the dataset from that point on.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group the songs in the dataset by the year of release and display `mean` and/or `max` duration of the songs in each year, as well as the number (`count`) of songs in each year (`<pd.df>.groupby('<column>').<another column>.<f()>.sort_values(ascending=False)` (aggregate using function `f()`, e.g. `mean()`), `<pd.df>.groupby('<column>').<another column>.agg(['f1 name>', '<f2 name>', ...])` (aggregate using multiple functions, e.g. `mean()`, `count()`, `max()`,...)).\n",
    "If `sort_values()` is used after `agg([<'f1 name>', '<f2 name>', ...])` (`agg(['<f1 name>', '<f2 name>', ...]).sort_values(by='<f name>', ascending=False)`), it must be passed one positional argument (`by='<f name>'`) before the optional `ascending=False`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make the groupings and aggregations\n",
    "\n",
    "# <pd.df>.groupby('<column>').<another column>.<f()>.sort_values(ascending=False)\n",
    "\n",
    "# <pd.df>.groupby('<column>').<another column>.agg(['<f1 name>', '<f2 name>', ...]).sort_values(by='<f name>', ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data visualization\n",
    "Plot some scatterplots, line plots, bar graphs, histograms, scatterplots, box plots, violins, heatmaps,...\n",
    "[Example](https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/)\n",
    "\n",
    "[Matplotlib examples](https://matplotlib.org/stable/gallery/index.html)\n",
    "\n",
    "[Seaborn examples](https://seaborn.pydata.org/examples/index.html) (see also [The Python Graph Gallery](https://www.python-graph-gallery.com/); it has a very neat user interface!)\n",
    "\n",
    "<u>**Note that it is also possible to**</u> <u>**[plot lines, bargraphs,... with Pandas only](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.line.html)**</u> (although in such cases Pandas interacts with Matplotlib under the hood)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<b>IMPORTANT: Matplotlib terminology, Figure vs. Axes</b><br>\n",
    "A `Figure` object in Matplotlib is the outermost container for a Matplotlib graphic, which can contain multiple `Axes` objects. One source of confusion is the name: an `Axes` actually translates into what we think of as an individual plot or graph (rather than the plural of \"axis\", as we might expect)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Missing values"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the dataset.\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the songs as a pd.DataFrame object from 'data/grunge_v4.csv', \n",
    "# or from '../data/grunge_v4.csv', or '../../data/grunge_v4.csv', or ...,\n",
    "# depending on where the csv file is located\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for missing values (use, e.g., `sb.heatmap(<pd.df>.isna(), cbar=False, cmap='viridis')`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Briefly analyze the rows with `NaN`s. To select all such rows, use `any()` (`<pd.df>.loc[<pd.df>.isna().any(axis=1), ['<column 1>', '<column 2>', ...]`). To select the rows where there are no `NaN`s at all, use `<pd.df>.loc[<pd.df>.notna().all(axis=1), ['<column 1>', '<column 2>', ...]`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If it is difficult to spot any regular pattern, get rid of `NaN`s in the simplest way possible (`<pd.df>.dropna(inplace=True)`). Make sure that the modified dataset is `NaN`-free (`<pd.df>.isna().sum()`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the reduced dataset as `grunge_visualization.csv`, the starting one to make visualizations."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Save the modified dataset\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Scatterplot"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the modified dataset (if necessary).\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scatterplot the relationship between `duration` and `danceability`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If the format of `duration` is `str`, change it to `int`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the modified dataset as `grunge_visualization_duration_int.csv`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Save the modified dataset\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To set the ranges of values on x-axis and y-axis (`duration`, `danceability`), check the max values or run `describe()`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### 1. Plotting using Matplotlib"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[Matplotlib scatterplot example](https://matplotlib.org/stable/gallery/shapes_and_collections/scatter.html)<br>\n",
    "[Excellent tutorial on matplotlib](https://realpython.com/python-matplotlib-guide/)\n",
    "\n",
    "Simple version:<br>\n",
    "`ax = plt.axes()`<br>\n",
    "`ax.set(xlim=(<from>, <to>), ylim=(<from>, <to>), xlabel='<xlabel>', ylabel='<ylabel>', title='<title>')`<br>\n",
    "`ax.scatter(<pd.df>['<X>'], <pd.df>['<Y>'], marker='<marker type>', c='<fill color>', edgecolors='<edgecolor>', s=<marker size>)`; <br>\n",
    "\n",
    "The `<pd.df>['<X>']` and `<pd.df>['<Y>']` arguments can be also specified as `<pd.df>.<X>` and `<pd.df>.<Y>` if `<X>` and `<Y>` are single words.\n",
    "The color parameter (`c`) is optional; if present, it should be a scalar or a sequence of length consistent with the lengths of `<X>` and `<Y>` (`(<X>, <Y>)` points). The `marker` parameter is optional as well. Both `c` and `marker` have defaults. For other values of `c` and `marker`, see [this](https://matplotlib.org/stable/gallery/color/named_colors.html#css-colors) and [this](https://matplotlib.org/stable/api/_as_gen/matplotlib.markers.MarkerStyle.html#matplotlib.markers.MarkerStyle.markers), respectively. A good value for `s` is 30-40 for 200-300 markers on the plot.\n",
    "\n",
    "Alternatively:<br>\n",
    "`ax.plot(<pd.df>['<X>'], <pd.df>['<Y>'], marker='<marker type>', color='<color>', linestyle='');`<br>\n",
    "\n",
    "The `linestyle=''` parameter is essential for plotting the dots only - omitting it means that the connecting lines are plotted as well.<br><br>\n",
    "\n",
    "Elaborated version, using plt.subplots():\n",
    "\n",
    "Get the Figure and the Axes objects\n",
    "\n",
    "`fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', facecolor='color', figsize=(<x_size>, <y_size>))`\n",
    "\n",
    "Plot the data on the Axes\n",
    "\n",
    "`ax.scatter(x=<x>, y=<y>, color='<color>', linewidth=<linewidth>, edgecolors='<edgecolors>', marker='<marker type>', s=<dot/point size>, alpha=<transparency>)`\n",
    "\n",
    "Set the Axes title, background color (face color), labels (incl. font sizes) and limits\n",
    "\n",
    "`ax.set_title('<title>', fontsize=12, loc='left')`<br>\n",
    "`ax.set_facecolor('<color>')`<br>\n",
    "`ax.set_xlabel('<x_label>', fontsize=8)`<br>\n",
    "`ax.set_ylabel('<y_label>', fontsize=8)`<br>\n",
    "`ax.set_xlim(<m>, <n>)`<br>\n",
    "`ax.set_ylim(<p>, <q>)`<br>\n",
    "\n",
    "Set the tick parameters<br>\n",
    "\n",
    "`ax.ticklabel_format(useOffset=False)`<br>\n",
    "`ax.tick_params(axis='x', labelsize=6)`<br>\n",
    "`ax.tick_params(axis='y', labelsize=6)`<br>\n",
    "\n",
    "Display the plot<br>\n",
    "\n",
    "`plt.show()`<br><br>\n",
    "\n",
    "In `fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', facecolor='color', figsize=(<x_size>, <y_size>))`, using `layout='constrained'` is recommended to avoid overlapping of figure elements when changing the figure size. For a good figure size, use `figsize=(3.5, 2)` or similar.\n",
    "\n",
    "To prevent numbers displayed in scientific notation (exponential) on axes ticks, make sure to use `ax.ticklabel_format(useOffset=False)`.\n",
    "\n",
    "Experiment with different font sizes for labels, title and ticks.\n",
    "\n",
    "The `color` parameter can be abbreviated as `c` and is optional; if present, it should be a scalar or a sequence of length consistent with the lengths of `<X>` and `<Y>` (`(<X>, <Y>)` points). The `marker` parameter is optional as well. Both `color` and `marker` have defaults. For other values of `color` and `marker`, see [this](https://matplotlib.org/stable/gallery/color/named_colors.html#css-colors) and [this](https://matplotlib.org/stable/api/_as_gen/matplotlib.markers.MarkerStyle.html#matplotlib.markers.MarkerStyle.markers), respectively. A good value for the dot/point size parameter `s` is 30-40 for 200-300 markers on the plot.\n",
    "\n",
    "Examples of some parameters in `ax.scatter()`: `color='steelblue'`, `linewidth=1` (the thickness of the dots/points rim), `alpha=0.8` (alpha: transparency (0-1))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Simple version\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Elaborated version, using plt.subplots()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### 2. A brief analysis of the plot: What are the shortest/longest songs and their durations?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display(<pd.df>['column'] <= <value>)                                    # Boolean mask\n",
    "# display(type(<pd.df>['column'] <= <value>))                              # pd.Series\n",
    "# display(<pd.df>[<pd.df>['column'] <= <value>]['column to to display'])   # select one column\n",
    "# display(<pd.df>[<pd.df>['column'] <= <value>]['column 1  to to display', 'column 2 to display',...])   # select multiple columns\n",
    "\n",
    "# Try this also with .loc[], as well as with .iloc[], with an explicitly set index and with .index\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Line plot"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the modified dataset (if necessary).\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How many songs from each `release_year` are there?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Use <pd.df>['<column>'].value_counts(), <pd.df>['<column>'].value_counts()[<specific value> in <column>]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sort this result by index: `pd.Series.sort_index()` (there is also `pd.DataFrame.sort_index()`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Define val_counts_sorted_by_index\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Preparation for plotting (`counts` on y-axis, `release_year` on x-axis): get the `np.ndarray` version of `val_counts_sorted_by_index`, as well as of its index.\n",
    "\n",
    "One way of doing it is to use `np.array()` over `val_counts_sorted_by_index.index` and `val_counts_sorted_by_index.values`. However, the same effect is achieved using only `val_counts_sorted_by_index.index` and `val_counts_sorted_by_index.values` (their type is `np.ndarray`)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now plot it."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### 1. Plotting using Matplotlib"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[Matplotlib line plot example](https://matplotlib.org/stable/gallery/lines_bars_and_markers/simple_plot.html)<br>\n",
    "[Excellent tutorial on matplotlib](https://realpython.com/python-matplotlib-guide/)\n",
    "<br>\n",
    "`ax = plt.axes()`<br>\n",
    "`ax.set(xlim=(<lower limit>, <upper limit>), ylim=(<lower limit>, <upper limit>), xlabel='...', ylabel='...', title='...')`<br>\n",
    "`ax.ticklabel_format(useOffset=False)`<br>\n",
    "`ax.plot(<x>, <y>, color='...', marker='<marker type>', linewidth=<number>, alpha=<number>)`<br>\n",
    "\n",
    "To prevent numbers displayed in scientific notation (exponential) on axes ticks, make sure to use `ax.ticklabel_format(useOffset=False)`.\n",
    "\n",
    "Do not use `x=<x>, y=<y>` in `ax.plot()`, it generates an error. Use just `<x>, <y>`. For the other parameters, the keywords are necessary.\n",
    "\n",
    "Examples of parameters in `ax.plot()`: `color='steelblue'`, `linewidth=3`, `alpha=0.8` (alpha: transparency (0-1))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Simple version\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<em>Elaborated version 1, without `plt.subplots()`</em><br><br>\n",
    "\n",
    "Set the Figure object parameters<br>\n",
    "\n",
    "`plt.figure(layout='constrained', facecolor='<color>', figsize=(<x_size>, <y_size>))`<br>\n",
    "\n",
    "Get the Axes object<br>\n",
    "\n",
    "`ax = plt.axes()`<br>\n",
    "\n",
    "Set the Axes object parameters<br>\n",
    "\n",
    "`ax.set_facecolor('<color>')`<br>\n",
    "`ax.set_title('<title>', fontsize=12, loc='left')`<br>\n",
    "`ax.set_xlabel('<x_label>', fontsize=8)`<br>\n",
    "`ax.set_ylabel('<y_label>', fontsize=8)`<br>\n",
    "\n",
    "Without `plt.subplots()`, `xlim` and `ylim` have to be set using `ax.set()`<br>\n",
    "\n",
    "`ax.set(xlim=(<m>, <n>), ylim=(<p>, <q>))`<br>\n",
    "\n",
    "Set the tick parameters<br>\n",
    "\n",
    "`ax.ticklabel_format(useOffset=False)`<br>\n",
    "`ax.tick_params(axis='x', labelsize=6)`<br>\n",
    "`ax.tick_params(axis='y', labelsize=6)`<br>\n",
    "\n",
    "Display the plot using `ax.plot()`<br>\n",
    "\n",
    "`ax.plot(<x>, <y>, color='<color>', linewidth=<linewidth>, alpha=<transparency>);`<br><br>\n",
    "\n",
    "In `plt.figure(layout='constrained', facecolor='<color>', figsize=(<x_size>, <y_size>))`, using `layout='constrained'` is recommended to avoid overlapping of figure elements when changing the figure size. For a good figure size, use `figsize=(3.5, 2)` or similar.\n",
    "\n",
    "It is also possible to set the Axes object background color using `plt.axes(facecolor='<color>')` instead of `ax.set_facecolor('<color>')`.\n",
    "\n",
    "To prevent numbers displayed in scientific notation (exponential) on axes ticks, make sure to use `ax.ticklabel_format(useOffset=False)`.\n",
    "\n",
    "Experiment with different font sizes for labels, title and ticks.\n",
    "\n",
    "**Do not use `x=<x>, y=<y>` in `ax.plot()`, it generates an error. Use just `<x>, <y>`. For the other parameters, the keywords are necessary.**\n",
    "\n",
    "Examples of parameters in `ax.plot()`: `color='steelblue'`, `linewidth=3`, `alpha=0.8` (alpha: transparency (0-1))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Elaborated version 1, without plt.subplots()\n",
    "\n",
    "# Set the Figure object parameters\n",
    "\n",
    "# Get the Axes object\n",
    "\n",
    "# Set the Axes object parameters\n",
    "\n",
    "# Without plt.subplots(), xlim and ylim have to be set using ax.set()\n",
    "\n",
    "# Set the tick parameters\n",
    "\n",
    "# Display the plot using ax.plot()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<em>Elaborated version 2, using `plt.subplots()`</em><br><br>\n",
    "\n",
    "Get the Figure and the Axes objects\n",
    "\n",
    "`fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', facecolor='color', figsize=(<x_size>, <y_size>))`\n",
    "\n",
    "Plot the data on the Axes\n",
    "\n",
    "`ax.plot(<x>, <y>, color='<color>', linewidth=<linewidth>, alpha=<transparency>)`\n",
    "\n",
    "Set the Axes title, background color (face color), labels (incl. font sizes) and limits\n",
    "\n",
    "`ax.set_title('<title>', fontsize=12, loc='left')`<br>\n",
    "`ax.set_facecolor('<color>')`<br>\n",
    "`ax.set_xlabel('<x_label>', fontsize=8)`<br>\n",
    "`ax.set_ylabel('<y_label>', fontsize=8)`<br>\n",
    "`ax.set_xlim(<m>, <n>)`<br>\n",
    "`ax.set_ylim(<p>, <q>)`<br>\n",
    "\n",
    "Set the tick parameters<br>\n",
    "\n",
    "`ax.ticklabel_format(useOffset=False)`<br>\n",
    "`ax.tick_params(axis='x', labelsize=6)`<br>\n",
    "`ax.tick_params(axis='y', labelsize=6)`<br>\n",
    "\n",
    "Display the plot<br>\n",
    "\n",
    "`plt.show()`<br><br>\n",
    "\n",
    "In `fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', facecolor='color', figsize=(<x_size>, <y_size>))`, using `layout='constrained'` is recommended to avoid overlapping of figure elements when changing the figure size. For a good figure size, use `figsize=(3.5, 2)` or similar.\n",
    "\n",
    "To prevent numbers displayed in scientific notation (exponential) on axes ticks, make sure to use `ax.ticklabel_format(useOffset=False)`.\n",
    "\n",
    "Experiment with different font sizes for labels, title and ticks.\n",
    "\n",
    "**Do not use `x=<x>, y=<y>` in `ax.plot()`, it generates an error. Use just `<x>, <y>`. For the other parameters, the keywords are necessary.**\n",
    "\n",
    "Examples of parameters in `ax.plot()`: `color='steelblue'`, `linewidth=3`, `alpha=0.8` (alpha: transparency (0-1))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Elaborated version 2, using plt.subplots()\n",
    "\n",
    "# Get the Figure and the Axes objects\n",
    "\n",
    "# Plot the data on the Axes\n",
    "\n",
    "# Set the Axes title, labels (incl. font sizes) and limits\n",
    "\n",
    "# Set the Axes background color (face color) \n",
    "\n",
    "# Set the tick parameters\n",
    "\n",
    "# Display the plot using plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### 2. Smoothen the curves\n",
    "Based on [this](https://stackoverflow.com/a/5284038/1899061).<br><br>\n",
    "`from scipy.interpolate import make_interp_spline, BSpline`<br>\n",
    "\n",
    "`<x> = <definition of x-axis variable>`<br>\n",
    "`<y> = <definition of y-axis variable>`<br>\n",
    "\n",
    "`<x_smooth> = np.linspace(<x>.min(), <x>max(), 300)`&emsp;&emsp;&emsp;&emsp;# 300: the number of points to make between `<x>.min() and <x>.max()`<br>\n",
    "`spl = make_interp_spline(release_year, counts, k=3)`&emsp;&emsp;&emsp;&emsp; # type: BSpline<br>\n",
    "`<y_smooth> = spl(<x>_smooth)`<br>\n",
    "\n",
    "`plt.xlim([<lowest value of x to show on the plot>, <highest value of x to show on the plot>])`<br>\n",
    "`plt.ylim([<lowest value of y to show on the plot>, <highest value of x to show on the plot>])`<br>\n",
    "\n",
    "`plt.plot(<x_smooth>, <y_smooth>)`<br>\n",
    "`plt.plot(<x>, <y>)`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;# optional: show the segmented line on the same plot as well<br>\n",
    "`plt.show()`\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 300 represents the number of points to make between T.min and T.max\n",
    "# T = np.array([6, 7, 8, 9, 10, 11, 12])\n",
    "# power = np.array([1.53E+03, 5.92E+02, 2.04E+02, 7.24E+01, 2.72E+01, 1.10E+01, 4.70E+00])\n",
    "#\n",
    "# # plt.plot(T,power)\n",
    "# # plt.show()\n",
    "#\n",
    "# xnew = np.linspace(T.min(), T.max(), 300)\n",
    "#\n",
    "# spl = make_interp_spline(T, power, k=3)  # type: BSpline\n",
    "# power_smooth = spl(xnew)\n",
    "#\n",
    "# plt.plot(xnew, power_smooth)\n",
    "# plt.show()\n",
    "\n",
    "# from scipy.interpolate import make_interp_spline, BSpline\n",
    "# \n",
    "# year_smooth = np.linspace(release_year.min(), release_year.max(), 300)\n",
    "# spl = make_interp_spline(release_year, counts, k=3)  # type: BSpline\n",
    "# counts_smooth = spl(year_smooth)\n",
    "# \n",
    "# # plt.figure(layout='constrained', figsize=(5, 3), facecolor='lightyellow', alpha=0.5)\n",
    "# fig, ax = plt.subplots(figsize=(10, 7), layout='constrained', facecolor='beige')\n",
    "# \n",
    "# ax.set_facecolor('navajowhite')\n",
    "# \n",
    "# plt.ticklabel_format(useOffset=False)\n",
    "# \n",
    "# plt.xlim([1989, 2020])\n",
    "# plt.ylim([0, 40])\n",
    "# plt.xticks(fontsize=8)\n",
    "# plt.yticks(fontsize=8)\n",
    "# \n",
    "# plt.xlabel('release_year', fontsize=10)\n",
    "# plt.ylabel('count', fontsize=10)\n",
    "# plt.title('Song counts over years', fontsize=12, color='green')\n",
    "# \n",
    "# plt.plot(year_smooth, counts_smooth)\n",
    "# plt.plot(release_year, counts)\n",
    "# plt.show()\n",
    "\n",
    "# # Alternatively\n",
    "# ax = plt.axes()\n",
    "# ax.set(xlim=(years.min()-1, years.max()+1), ylim=(150, 400), xlabel='release year', ylabel='count', title='Song counts over years')\n",
    "# ax.ticklabel_format(useOffset=False)\n",
    "# ax.plot(years, counts, color='steelblue', linewidth=2, marker='o', alpha=0.8)\n",
    "# ax.plot(year_smooth, counts_smooth, color='green', linewidth=2, alpha=0.8);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### 3. Multiple subplots\n",
    "(shown here after [this](https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # From https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html\n",
    "# fig = plt.figure()\n",
    "# ax1 = fig.add_axes([0.1, 0.55, 0.8, 0.4],\n",
    "#                    xticklabels=[], ylim=(-1.2, 1.2))\n",
    "# ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.4],\n",
    "#                    ylim=(-1.2, 1.2))\n",
    "# # Meanings of the numbers in [0.1, 0.55, 0.8, 0.4]:\n",
    "# #     0.1 - distance from the left edge of fig (grey area)\n",
    "# #     0.55 - distance between the upper and lower subplots (0.5: they touch each other)\n",
    "# #     0.8 - distance from the right edge of fig (grey area)\n",
    "# #     0.4 - area assigned to the upper/lower subplot (ax1/ax2) along the vertical axes\n",
    "# # Experiment with these numbers to get a better feeling for them\n",
    "\n",
    "# x = np.linspace(0, 10)\n",
    "# ax1.plot(np.sin(x))\n",
    "# ax2.plot(np.cos(x));\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 6), )\n",
    "# # fig\n",
    "# ax1 = fig.add_axes([0.1, 0.579, 0.8, 0.35],\n",
    "#                    xlim=(1963, 1968), ylim=(150, 400),\n",
    "#                    xlabel='release year', ylabel='counts',\n",
    "#                    title='Number of songs recorded over the years')\n",
    "# ax2 = fig.add_axes([0.1, 0.08, 0.8, 0.35],\n",
    "#                    xlim=(1963, 1968), ylim=(150, 400),\n",
    "#                    xlabel='release year', ylabel='counts',\n",
    "#                    title='Number of songs recorded over the years')\n",
    "# # display(type(ax1))\n",
    "# ax1.ticklabel_format(useOffset=False)\n",
    "# ax2.ticklabel_format(useOffset=False)\n",
    "# \n",
    "# ax1.plot(years, counts, color='steelblue', linewidth=1.5, alpha=0.8)    # alpha: transparency (0-1)\n",
    "# ax2.plot(years, counts, color='purple', linewidth=1.5, alpha=0.8);      # alpha: transparency (0-1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # From https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html\n",
    "# fig = plt.figure()\n",
    "# ax1 = fig.add_axes([0.1, 0.55, 0.8, 0.4],\n",
    "#                    xticklabels=[], ylim=(-1.2, 1.2))\n",
    "# ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.4],\n",
    "#                    ylim=(-1.2, 1.2))\n",
    "# # Meanings of the numbers in [0.1, 0.55, 0.8, 0.4]:\n",
    "# #     0.1 - distance from the left edge of fig (grey area)\n",
    "# #     0.55 - distance between the upper and lower subplots (0.5: they touch each other)\n",
    "# #     0.8 - distance from the right edge of fig (grey area)\n",
    "# #     0.4 - area assigned to the upper/lower subplot (ax1/ax2) along the vertical axes\n",
    "# # Experiment with these numbers to get a better feeling for them\n",
    "\n",
    "# x = np.linspace(0, 10)\n",
    "# ax1.plot(np.sin(x))\n",
    "# ax2.plot(np.cos(x));\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 7), )\n",
    "# # fig\n",
    "# ax1 = fig.add_axes([0.1, 0.579, 0.8, 0.35],\n",
    "#                    xlim=(1989, 2020), ylim=(0, 40),\n",
    "#                    xlabel='release year', ylabel='counts',\n",
    "#                    title='Number of songs over the years')\n",
    "# ax2 = fig.add_axes([0.1, 0.08, 0.8, 0.35],\n",
    "#                    xlim=(1989, 2020), ylim=(0, 40),\n",
    "#                    xlabel='release year', ylabel='counts',\n",
    "#                    title='Number of songs over the years')\n",
    "# # display(type(ax1))\n",
    "# ax1.ticklabel_format(useOffset=False)\n",
    "# ax2.ticklabel_format(useOffset=False)\n",
    "# \n",
    "# ax1.plot(release_year, counts, color='steelblue', linewidth=1.5, alpha=0.8)    # alpha: transparency (0-1)\n",
    "# ax2.plot(release_year, counts, color='purple', linewidth=1.5, alpha=0.8);      # alpha: transparency (0-1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Histogram"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Read the dataset (`grunge_visualization.csv`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the histogram of song durations (lengths, times)."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use Pandas to extract song lengths as a `pd.Series` object (`<pd.Series object> = <pd.df>['<column>']`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Get the song lengths as a pd.Series object\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Convert the song lengths into a NumPy array (using <song lengths>.to_numpy() or np.array(<song lengths>))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Plotting using Matplotlib"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[Matplotlib histogram example](https://matplotlib.org/stable/gallery/statistics/hist.html)\n",
    "\n",
    "Plot the histogram of the song lengths using Matplotlib.\n",
    "\n",
    "Minimal version: `plt.hist(<x>, bins=<number of bins>);` or `sb.histplot(<x>, bins=<number of bins>)`.\n",
    "\n",
    "Alternatively:<br>\n",
    "`plt.figure(layout='constrained', facecolor='<color>', figsize=(3.5, 2), )`<br>\n",
    "`ax = plt.axes()`<br>\n",
    "`ax.set(xlabel='...', ylabel='...', title='...')`<br>\n",
    "`ax.hist(<x>, bins=<number of bins>)`<br>\n",
    "\n",
    "As for the plot styles, there are a lot of [available styles](https://www.dunderdata.com/blog/view-all-available-matplotlib-styles) that can be also shown in code using `plt.style.available`. See also [this](https://www.analyticsvidhya.com/blog/2021/08/exploring-matplotlib-stylesheets-for-data-visualization/).\n",
    "\n",
    "Alternatively, plot style can be set using `sb.set_theme(palette='...')` (or just `sb.set()`, but that function might get deprecated and removed from *Seaborn* in the future). See [`sb.set_theme()` documentation](https://seaborn.pydata.org/generated/seaborn.set_theme.html) for the function's parameters and defaults. For `palette='...'` use any of the palettes shown with `plt.style.available`, or any of [these](https://matplotlib.org/stable/users/explain/colors/colormaps.html#qualitative), or...\n",
    "\n",
    "Elaborated version, using plt.subplots():\n",
    "\n",
    "Get the Figure and the Axes objects\n",
    "\n",
    "`fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', facecolor='color', figsize=(<x_size>, <y_size>))`\n",
    "\n",
    "Plot the data on the Axes\n",
    "\n",
    "`ax.hist(x=<x>, bins=<number of bins>, color='<color>', linewidth=<linewidth>, edgecolor='<edgecolors>', alpha=<transparency>)`\n",
    "\n",
    "Set the Axes title, background color (face color), labels (incl. font sizes) and limits\n",
    "\n",
    "`ax.set_title('<title>', fontsize=12, loc='left')`<br>\n",
    "`ax.set_facecolor('<color>')`<br>\n",
    "`ax.set_xlabel('<x_label>', fontsize=8)`<br>\n",
    "`ax.set_ylabel('<y_label>', fontsize=8)`<br>\n",
    "`ax.set_xlim(<m>, <n>)`<br>\n",
    "`ax.set_ylim(<p>, <q>)`<br>\n",
    "\n",
    "Set the tick parameters<br>\n",
    "\n",
    "`ax.ticklabel_format(useOffset=False)`<br>\n",
    "`ax.tick_params(axis='x', labelsize=6)`<br>\n",
    "`ax.tick_params(axis='y', labelsize=6)`<br>\n",
    "\n",
    "Display the plot<br>\n",
    "\n",
    "`plt.show()`<br><br>\n",
    "\n",
    "In `fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', facecolor='color', figsize=(<x_size>, <y_size>))`, using `layout='constrained'` is recommended to avoid overlapping of figure elements when changing the figure size. For a good figure size, use `figsize=(3.5, 2)` or similar.\n",
    "\n",
    "To prevent numbers displayed in scientific notation (exponential) on axes ticks, make sure to use `ax.ticklabel_format(useOffset=False)`.\n",
    "\n",
    "Experiment with different font sizes for labels, title and ticks.\n",
    "\n",
    "The `color` parameter can be abbreviated as `c` and is optional. The `marker` parameter is optional as well, and so is `edgecolor` (can be abbreviated as `ec`). They all have defaults. For other values of `color` and `marker`, see [this](https://matplotlib.org/stable/gallery/color/named_colors.html#css-colors) and [this](https://matplotlib.org/stable/api/_as_gen/matplotlib.markers.MarkerStyle.html#matplotlib.markers.MarkerStyle.markers), respectively.\n",
    "\n",
    "Examples of some parameters in `ax.hist()`: `color='steelblue'`, `linewidth=1` (the thickness of the dots/points rim), `alpha=0.8` (alpha: transparency (0-1))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set plot style using sb.set_theme(palette='Pastel2')\n",
    "\n",
    "# Plot the histogram - x: song time in [sec]; y: number of songs; 40 bins\n",
    "\n",
    "# Minimal version\n",
    "\n",
    "# A more detailed version\n",
    "\n",
    "# Elaborated version, using plt.subplots()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Bar graph"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the dataset (`'../data/grunge_visualization.csv'`) and make some minor transformations.\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the songs as a pd.DataFrame object from 'data/grunge_visualization.csv', or from\n",
    "# '../data/grunge_visualization.csv', or '../../data/grunge_visualization.csv', or ..., \n",
    "# depending on where the csv file is located\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How many powerful, energetic, intense, loud, and possibly anthemic songs did each grunge band released during the grunge period?"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define a new feature (column in the `songs` dataframe), `powerful`, as a combination of `energy` and `loudness` - songs with `energy` and `loudness` above the corresponding 3rd quartiles are considered powerful."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Run songs.describe() to see the 3rd quartiles\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the 3rd quartiles for selected candidate features to describe the new feature, 'poweful' \n",
    "# ('danceability', 'energy', 'liveness', 'loudness', 'tempo', 'valence')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Define threshold values for the candidate features (3rd quartiles, i.e. '75%')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the condition for a song to be powerful (songs with `energy` and `loudness` above the corresponding 3rd quartiles); \n",
    "# experiment with different combinations of candidate features\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Define the new feature, 'powerful'\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Display these powerful songs and their performers\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How many grunge songs have been powerful, in terms of the definition of `songs.powerful` shown above?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<u>Save this version as a new *.csv* file, for use in the subsequent examples.</u> (`<pd.df>.to_csv('<path>')`)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Preparing the data for plotting the bar graph"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Group the data - group the songs by artists."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use `get_group(<artists>)` to get all songs by a selected performer and `value_counts()` over the resulting group's `powerful` column (showing the `True` and `False` subgroups). This is a precursor to creating the data for the y-axis of the bar graph."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Build the data to plot by extracting relevant items from each group."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For x-axis, use `unique()` over the `artists` column, and then optionally `list()` over the resulting array to make the list of artists."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For y-axis, create the lists of the numbers of powerful songs (`powerful`) and of the other ones (`not_powerful`).\n",
    "(Start from two empty lists. Loop over the list of artistss created in the previous step, `get_group()` for each artist/band and append the `value_counts()[True]` of the `powerful` column of the current artist (`a['powerful']`) to `powerful` if any of `a['powerful']` has the value `True`, otherwise append 0. Do the similar thing for `not_powerful`. Display both lists in the end to double-check the result.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now plot the bar graph. Based on the second example from [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html) (using `<pd.df>.plot.bar()`, not Matplotlib or Seaborn).\n",
    "For a complete list of parameters used in `**kwargs`, see [this](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html).\n",
    "For a list of named colors (Matplotlib named colors), see [here](https://matplotlib.org/stable/gallery/color/named_colors.html#css-colors)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First create an auxiliary dataframe to use for plotting. Use `pwerful` and `not_powerful` as the columns, <u>and the list of artists created above as the index of the dataframe</u>."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # The role-model example from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\n",
    "# speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n",
    "# lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n",
    "# index = ['snail', 'pig', 'elephant', 'rabbit', 'giraffe', 'coyote', 'horse']\n",
    "# df = pd.DataFrame({'speed': speed, 'lifespan': lifespan}, index=index)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Alternative 1 - plotting the bargraph using Pandas (`<pd.df>.plot.bar()`)\n",
    "\n",
    "[Pandas bargraph example](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.bar.html)\n",
    "\n",
    "Use `ax = <pd.df>.plot.bar()` to plot the bargraph.\n",
    "\n",
    "Relevant parameters:\n",
    "- `figsize=(<width>, <height>)` (e.g., (6, 6))\n",
    "- `rot=<rotation angle [degrees]>` for the x-axis labels\n",
    "- `ylim=(<from>, <to>)`\n",
    "- `color={'powerful': 'limegreen', 'not_powerful': 'navajowhite'}` (for a list of Matplotlib named colors, see [here](https://matplotlib.org/stable/gallery/color/named_colors.html#css-colors))\n",
    "- `edgecolor='<color of bin lines>'`\n",
    "- `title='<title>'`\n",
    "- `xlabel='<xlabel>'`\n",
    "- `ylabel='<ylabel>'`\n",
    "- `fontsize=<fontsize>` (for all text; suitable fontsizes are 10, 12,...)\n",
    "- `stacked=True` (the bins for the same x-axis value stacked on top of one another)\n",
    "\n",
    "To show labels (counts) in each bin container, use:\n",
    "\n",
    "`for c in ax.containers:`<br>\n",
    "&emsp;&emsp;`ax.bar_label(c, label_type='center')`\n",
    "\n",
    "If the labels (counts) in each container are not needed, the returned value (`ax`) is usually unnecessary and can be omitted.\n",
    "\n",
    "It is <b>a very good idea</b> to also use `plt.tight_layout()` <b>after</b> `<pd.df>.plot.bar()` to avoid cutoffs at the bottom of the figure.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Alternative 2 - plotting the bargraph using Seaborn (`sb.countplot()`)\n",
    "\n",
    "Use `ax = sb.countplot()` to plot the bargraph.\n",
    "\n",
    "Relevant parameters:\n",
    "- `data=<pd.df>`\n",
    "- `x='<column 1>'` (e.g., 'artists')\n",
    "- `hue='<column 2>'` (e.g., 'powerful`)\n",
    "- `palette='<palette>'` (e.g., 'Set2'; it is also possible to define custom palletes using Hex codes, e.g. `palette=['#432371','#FAAE7B']`)\n",
    "- `dodge=False` to make the bargraph **seemingly stacked** (the containers are not on top of each other, they just overlap); if no stacking is needed (which is a recommended option here), hust omit `dodge=False`\n",
    "\n",
    "**Making the bargraph really stacked (its containers on top of each other) uses (`dodge=False`), but it cannot be done directly (no kwarg for that, i.e. no specific feature for that in `Seaborn.countplot()`), only with some extra work** (see a possible solution [here](https://stackoverflow.com/a/67116235/1899061)).\n",
    "\n",
    "If necessary, use `plt.xticks(rotation=<rotation angle [degrees]>)` before `sb.countplot()`.\n",
    "\n",
    "Note that `ax = sb.countplot()` returns a `pd.Axes` object, so after the call to `ax = sb.countplot()` all `pd.Axes` methods can be called (like `ax.set_title(title='<title>'`, `ax.set_ylim(...)`, etc.). \n",
    "\n",
    "To show labels (counts) in each bin container, use:\n",
    "\n",
    "`for c in ax.containers:`<br>\n",
    "&emsp;&emsp;`ax.bar_label(c, label_type='center')`\n",
    "\n",
    "If the labels (counts) in each container are not needed, the returned value (`ax`) is usually unnecessary and can be omitted.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.figure(figsize=(8, 6), facecolor='navajowhite')\n",
    "# plt.xticks(rotation=90)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Box plot\n",
    "[Seaborn boxplot example](https://seaborn.pydata.org/generated/seaborn.boxplot.html) (used here as the role model)\n",
    "\n",
    "For Seaborn color palette names see [this](https://seaborn.pydata.org/generated/seaborn.color_palette.html#seaborn.color_palette) or [this](https://10xsoft.org/courses/data-analysis/mastering-data-visualization-with-python/section-4-data-visualization-using-seaborn/colour-palettes-seaborn/). To list the names of some ('quantitative') Seaborn color palettes, use `sb.palettes.SEABORN_PALETTES.keys()` (see [this](https://10xsoft.org/courses/data-analysis/mastering-data-visualization-with-python/section-4-data-visualization-using-seaborn/colour-palettes-seaborn/) and [this](https://www.codecademy.com/article/seaborn-design-ii) for additional named palettes)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the dataset (`'data/grunge_visualization.csv'`).\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the songs as a pd.DataFrame object from 'data/grunge_visualization.csv', or from\n",
    "# '../data/grunge_visualization.csv', or '../../data/grunge_visualization.csv', or ..., depending on where the csv file is located\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use `sb.boxplot(y='<column>' | x='<column>', palette='<palette>', hue=1, legend=False)` to plot some boxplots.\n",
    "\n",
    "For a single-column boxplot, relevant parameters are `y=<pd.df>['column']` (for 'vertical' boxplot) or `x=<pd.df>['column']` (for 'horizontal' boxplot), and `palette='<palette>'` (e.g., 'Set3', 'pastel', ...; see the links above for other named color palettes). <u>Note that in case `palette` is used, it is also necessary to use `hue=<n>`, where `<n>` can be any value, e.g. 1</u>.\n",
    "\n",
    "For a multiple-column boxplot, relevant parameters are `data=<pd.df>[['column1', 'column2',...]]`, `orient='v'` (for 'vertical' boxplot) and `palette='<palette>'`. No `hue` is needed, no `legend`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display(sb.palettes.SEABORN_PALETTES.keys())\n",
    "\n",
    "# plt.figure(layout='constrained', facecolor='navajowhite', figsize=(3.5, 2), )\n",
    "\n",
    "# For a single column (e.g., duration)\n",
    "# sb.boxplot(y=songs.duration, palette='Set1', hue=1, legend=False)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Alternatively\n",
    "# sb.boxplot(data=songs, y='duration', palette='Set1', hue=1, legend=False)\n",
    "\n",
    "# For multiple columns (e.g., energy and acousticness)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Violin plot\n",
    "[Seaborn violin plot example](https://seaborn.pydata.org/generated/seaborn.violinplot.html)\n",
    "\n",
    "Combines box plot and density plot. Based on [this](https://stackoverflow.com/questions/46134113/seaborn-violin-plot-from-pandas-dataframe-each-column-its-own-separate-violin-p) and [this](https://seaborn.pydata.org/generated/seaborn.violinplot.html)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the dataset (`'../data/grunge_visualization.csv'`).\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the songs as a pd.DataFrame object from ''../data/grunge_visualization.csv'', or from\n",
    "# '../data/grunge_visualization.csv', or '../../data/grunge_visualization.csv', or ..., depending on where the csv file is located\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use `sb.violinplot()` like: `x=<pd.df>.loc[<index>, '<column for x-axis>']`, `sb.violinplot(data=<pd.df>, x=x, y=<pd.df>['<column for y-axis>'], hue=x, palette='<palette>', legend=False)`.\n",
    "\n",
    "For example, if the violin plot should represent density/boxplot diagram of song `duration` in certain `release_year`s, then `<column for x-axis>` is `release_year` and `<column for y-axis>` is `duration`. Good values for `'<palette>'` are, e.g., 'Set3', 'pastel',...).\n",
    "\n",
    "It is a good practice to set the `x` parameter directly before the call to `sb.violinplot()`, and then use `x=x` in `sb.violinplot()`. Using `x=<pd.df>.loc[<index>, '<column for x-axis>']` within the call to `sb.violinplot()` (like call to `sb.violinplot(x=<pd.df>.loc[<index>, '<column for x-axis>'], y=..., ...)`) might generate an error."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.figure(layout='constrained', facecolor='navajowhite', figsize=(5, 3), )\n",
    "\n",
    "# # Alternatively, set x and y explicitly and then call sb.violinplot() as\n",
    "# # sb.violinplot(data=songs, x=x, y=y, hue=x, palette='Set1', legend=False);\n",
    "# x=songs.loc[songs.release_year < 1995, 'release_year']\n",
    "# y=songs.duration\n",
    "# sb.violinplot(data=songs, x=x, y=y, hue=x, palette='Set1', legend=False);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Heat map\n",
    "[Seaborn heat map example](https://seaborn.pydata.org/generated/seaborn.heatmap.html) (used here as the role model)\n",
    "\n",
    "To create a heatmap, create the corresponding pivot table first. [An intuitive visual explanation of pivot tables](https://support.microsoft.com/en-us/office/overview-of-pivottables-and-pivotcharts-527c8fa3-02c0-445a-a2db-7794676bce96#:~:text=A%20PivotTable%20is%20an%20interactive,unanticipated%20questions%20about%20your%20data.) (start from [this raw table](https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576), and then see [the corresponding pivot table](https://support.microsoft.com/en-us/office/overview-of-pivottables-and-pivotcharts-527c8fa3-02c0-445a-a2db-7794676bce96#:~:text=A%20PivotTable%20is%20an%20interactive,unanticipated%20questions%20about%20your%20data.) (expand <em>About Pivot Tables</em>)).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Read the dataset (`'../data/grunge_visualization.csv'`).\n",
    "\n",
    "`pd.read_csv()` returns a `pd.DataFrame` object.\n",
    "\n",
    "As for specifying the path of the dataset properly, see [this](https://stackoverflow.com/questions/35384358/how-to-open-my-files-in-data-folder-with-pandas-using-relative-path) (more specifically, **both** [this](https://stackoverflow.com/a/35384414/1899061) and [this](https://stackoverflow.com/a/43600253/1899061))."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the songs as a pd.DataFrame object from ''../data/grunge_visualization.csv'', or from\n",
    "# '../data/grunge_visualization.csv', or '../../data/grunge_visualization.csv', or ..., depending on where the csv file is located\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The idea: categorize songs according to their *valence*."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Plot the density function for 'valence'\n",
    "# from plotnine import geom_density\n",
    "# (\n",
    "#     ggplot(songs, aes(x='valence')) +\n",
    "#     geom_density()\n",
    "# ).draw()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Alternative 1 - using `pd.qcut()`\n",
    "Create a new column in the dataframe, e.g. `valence_category`, using `pd.qcut()` function to split the entire range of `songs.valence` values into five equally sized subranges, `Very Low` to `Very High` (with ~equal number of elements in each subrange): `songs['valence_category'] = pd.qcut(songs.valence, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Create the new column\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the type of its values using type(<pd.df>.<new column>.values)\n",
    "\n",
    "# Display the categories in the new column using <pd.df>.<new column>.cat.categories\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Check value_counts() for 'valence_category' (the counts should be more or less equal)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Alternative 2 - using `pd.cut()`\n",
    "Create a new column in the dataframe, e.g. `valence_category`, using `pd.cut()` function to split the entire range of `songs.valence` values into five subranges, `Very Low` to `Very High` (with  generally *unequal* number of elements in each subrange): `songs['valence_category'] = pd.cut(songs.valence, bins=[<bin edges>], labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'], include_lowest=True)`. Depending on the value of `bins` parameter, the subranges are either equally *spaced* (`bins=<number of subranges>`), or the edges of each subrange interval are specified in `bins` explicitly. Note that there must be one more `<bin edges>` than bins (defined in `labels`). \n",
    "\n",
    "Note also that the ranges of values in the bins are defined as `(...]`. Thus make sure to include `include_lowest=True` in the call to `pd.cut()` to include the lowest value in the first bin (i.e., to get its range as `[...]`, not as `(...]`). The highest value in the last bin is always included."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Extract mean, median and other values of valence as v_mean, v_median, etc. from songs.valence.describe().values, to be used as bin edges\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the list of bin edges (v_min, v_mean, v_median, etc.)\n",
    "\n",
    "# Dafine the list of bin labels ('Very Low','Low', etc.)\n",
    "\n",
    "# Create 'valence_category' using pd.cut(songs['valence'], ...)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the type of its values using type(<pd.df>.<new column>.values)\n",
    "\n",
    "# Display the categories in the new column using <pd.df>.<new column>.cat.categories\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Check value_counts() for 'valence_category' (in general, the counts should NOT be equal)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### Alternative 3 - create `valence` categories manually\n",
    "For example, split the range of `valence` to five subranges, `Very Low` to `Very High` according to the following criteria: \n",
    "- `Very Low` is the *valence* from 0 to the first quartile (`songs.valence.describe()['25%']`)\n",
    "- `Low` is the *valence* from the first quartile to the mean value (`songs.valence.describe()['mean']`), since the mean value is lower than the median value\n",
    "- `Medium` is the *valence* from the mean value to the median value (`songs.valence.describe()['50%']`)\n",
    "- `High` is the *valence* from the median value to the third quartile (`songs.valence.describe()['75%']`)\n",
    "- `Very High` is the *valence* from the third quartile to 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Extract mean, median and other values of valence as v_mean, v_median, etc. from songs.valence.describe().values\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Insert a new column, e.g. `valence_category` and set it to the default value `Medium`. Then split the range of `valence` to five subranges, `Very Low` to `Very High` (find the `max()` of `valence` first). Each such a subrange is actually an index of selected songs, based on the value of `valence` (e.g., `very_low = songs['valence'] < 10`). Then use `<pd.df>.loc[<index of selected observations>, <relevant column>]` to change the default value `Medium` where appropriate (e.g., `songs.loc[very_low, 'valence_category'] = 'Very Low'`)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Insert a new column, e.g. valence_category and set it to the default value 'Medium'. \n",
    "# Then split the range of valence to five subranges, 'Very Low' to 'Very High.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<u>Save this version as a new *.csv* file, e.g. `grunge_visualization_valence_categories.csv`, for possible use in other examples.</u> (`<pd.df>.to_csv('<path>')`)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Rearrange the categories of `valence_category` to make the output natural.\n",
    "Use `<pd.df>['<column>'] = pd.Categorical(<pd.df>[<column>], categories=['<cat1>, <cat2>, ...'], ordered=True)`. In this example, order categories from `Very High` to `Very Low`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create a suitable pivot table. Use `<pivot table> = <pd.df>.pivot_table(values='<column with values to show on the heatmap>', index='<categorical index>', columns='<column>')`\n",
    "- `values`: e.g. `acousticness` or `tempo`\n",
    "- `index`: to be shown on y-axis, e.g. `valence_category`\n",
    "- `columns`: to be shown on x-axis, e.g. `release_year`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pivot_table = songs.pivot_table(values='energy', index='valence_category', columns='release_year')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Plot the corresponding heatmap. Based on [this](https://pythonbasics.org/seaborn-heatmap/), [this](https://seaborn.pydata.org/generated/seaborn.heatmap.html), and [this](https://stackoverflow.com/a/29648332/1899061).\n",
    "\n",
    "It is often a good idea to change the default figure size first, using `sb.set_theme(rc={'figure.figsize': (<x_size>, <y_size>)})`, to avoid cluttering on the heatmap (alternatively, use something like `plt.figure(layout='constrained', facecolor='navajowhite', figsize=(5, 3.5))`). Here `rc` stands for 'run command' - essentially, configurations which will execute when running the code. Experiment with `(<x_size>, <y_size>)`. The values that have worked well in this example: (15.7, 5.27).\n",
    "\n",
    "Then use `sb.heatmap(data=<pivot table>, annot=True, fmt='<format string>', cmap='<color map>');`\n",
    "- `data=<pivot table>`: the pivot table created in the previous step\n",
    "- `annot=True`: annotate heatmap cells with values\n",
    "- `fmt='<format_string>'`: for example, use `'.0f'` to show int values in annotations, not scientific notation (`'g'` for using mixed int and float annotations)\n",
    "- `cmap='<color map>'`: color map (see [this](https://10xsoft.org/courses/data-analysis/mastering-data-visualization-with-python/section-4-data-visualization-using-seaborn/colour-palettes-seaborn/)); a good one is `viridis`\n",
    "\n",
    "To set the title for the heatmap, or to change the axes labels, use (<b>AFTER</b> the call to `sb.heatmap()`!) something like:\n",
    "\n",
    "`plt.title('<title>', loc='left', color='<color>', alpha=0.4, size=14)`<br>\n",
    "`plt.xlabel('<xlabel>', size=<font size>, color='<color>')`<br>\n",
    "`plt.ylabel('<ylabel>', size=<font size>, color='<color>')`<br>\n",
    "`plt.show()`    # it's a must"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sb.set_theme(rc={'figure.figsize': (15.7, 5.27)})\n",
    "# plt.figure(layout='constrained', facecolor='navajowhite', figsize=(5, 3.5))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### A fancier example\n",
    "Average duration of songs over the years, represented as circles with sizes proportional to the numbers of songs."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# songs = pd.read_csv('../data/grunge_visualization_valence_categories.csv')\n",
    "# songs_by_year = songs.groupby('release_year')\n",
    "# years = np.sort(songs.release_year.unique())\n",
    "# years\n",
    "# \n",
    "# avg_duration = []\n",
    "# for year in years:\n",
    "#     avg_duration.append(np.mean(songs_by_year.get_group(year)['duration']))\n",
    "# avg_duration = np.array(avg_duration)\n",
    "# \n",
    "# rng = np.random.RandomState(370)\n",
    "# \n",
    "# colors = rng.choice(100, size=len(years), replace=False)                    # random sample, no duplicates\n",
    "# # display(colors)\n",
    "# \n",
    "# sizes = []\n",
    "# for year in years:\n",
    "#     sizes.append(len(songs_by_year.get_group(year)) * 100)                  # sizes proportional to the numbers of songs\n",
    "# \n",
    "# # plt.title('Song duration over the years', fontdict={'size': 20})\n",
    "# # plt.xlabel('release year')\n",
    "# # plt.ylabel('duration')\n",
    "# # plt.xlim(1989, 2020)\n",
    "# # plt.ticklabel_format(useOffset=False)\n",
    "# # plt.scatter(years, avg_duration,\n",
    "# #             c=colors, s=sizes, alpha=0.3,                                   # alpha: the level of transparency\n",
    "# #             cmap='Set1')                                                    # cmap: a pre-defined color map\n",
    "# # plt.colorbar();                                                             # show color scale\n",
    "# # \n",
    "# # # Alternatively, but without showing the colorbar\n",
    "# # ax = plt.axes()\n",
    "# # ax.set(xlabel='release year', ylabel='duration', xlim=(1989, 2020),\n",
    "# #        title='Song duration over the years')\n",
    "# # plt.ticklabel_format(useOffset=False)\n",
    "# # ax.scatter(years, avg_duration,\n",
    "# #            c=colors, s=sizes, alpha=0.3,                                    # alpha: the level of transparency\n",
    "# #            cmap='Set1');                                                    # cmap: a pre-defined color map"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
